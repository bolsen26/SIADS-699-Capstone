{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install praw\n",
    "nltk.download('stopwords')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import praw\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "from nltk.util import bigrams\n",
    "import string\n",
    "import warnings\n",
    "\n",
    "warnings.simplefilter(action='ignore', category=DeprecationWarning)\n",
    "warnings.simplefilter(action='ignore', category=UserWarning)\n",
    "\n",
    "# Initialize the Reddit API client\n",
    "reddit = praw.Reddit(\n",
    "    client_id=\"uTp5HipGLhBhPv1S7oxk8A\",\n",
    "    client_secret=\"AcT-azqNdIT3onTltclkX4Q3ISUDDQ\",\n",
    "    user_agent=\"mads\",\n",
    ")\n",
    "\n",
    "# Fetch top 100 posts from the 'netflix' subreddit from the past month\n",
    "subreddit = reddit.subreddit('netflix')\n",
    "top_posts = subreddit.top('month', limit=100)\n",
    "\n",
    "# Define stopwords to be removed from analysis\n",
    "stop_words = set(stopwords.words('english'))\n",
    "custom_stopwords = {\"netflix\", \"season\", \"new\", \"shows\", \"series\", \"show\", \"movie\", \"movies\",\"streaming\"}\n",
    "stop_words = stop_words.union(custom_stopwords)\n",
    "\n",
    "# Gather titles and comments\n",
    "all_texts = []\n",
    "for post in top_posts:\n",
    "    all_texts.append(post.title)\n",
    "    for comment in post.comments:\n",
    "        if isinstance(comment, praw.models.MoreComments):\n",
    "            continue\n",
    "        all_texts.append(comment.body)\n",
    "all_texts = \" \".join(all_texts).lower()\n",
    "\n",
    "# Tokenize and filter stopwords\n",
    "tokens = [word.strip(string.punctuation) for word in all_texts.split()]\n",
    "filtered_tokens = [word for word in tokens if word not in stop_words and len(word) > 1]\n",
    "\n",
    "# Create bigrams\n",
    "bi_grams = list(bigrams(filtered_tokens))\n",
    "\n",
    "# Count them\n",
    "bi_gram_freq = Counter(bi_grams)\n",
    "\n",
    "\n",
    "# Visualizing results for bigrams\n",
    "top_bi_grams = bi_gram_freq.most_common(20)\n",
    "bi_gram_strings, bi_gram_counts = zip(*[(f\"{word1} {word2}\", count) for (word1, word2), count in top_bi_grams])\n",
    "\n",
    "plt.figure(figsize=(15,10))\n",
    "plt.barh(bi_gram_strings, bi_gram_counts, color='skyblue')\n",
    "plt.xlabel('Counts')\n",
    "plt.title('Top bi-grams in Netflix Subreddit Past Month')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.show()\n",
    "\n",
    "# Print results for bi-grams\n",
    "for bi_gram, count in top_bi_grams:\n",
    "    print(f\"{bi_gram[0]} {bi_gram[1]}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import praw\n",
    "import matplotlib.pyplot as plt\n",
    "from textblob import TextBlob\n",
    "import warnings\n",
    "\n",
    "warnings.simplefilter(action='ignore', category=DeprecationWarning)\n",
    "warnings.simplefilter(action='ignore', category=UserWarning)\n",
    "\n",
    "# Initialize the Reddit API client\n",
    "reddit = praw.Reddit(\n",
    "    client_id=\"uTp5HipGLhBhPv1S7oxk8A\",\n",
    "    client_secret=\"AcT-azqNdIT3onTltclkX4Q3ISUDDQ\",\n",
    "    user_agent=\"mads\",\n",
    ")\n",
    "\n",
    "# Fetch top 100 posts from the 'netflix' subreddit from the past month\n",
    "subreddit = reddit.subreddit('netflix')\n",
    "top_posts = subreddit.top('month', limit=100)\n",
    "\n",
    "# Extract comments mentioning \"Breaking Bad\"\n",
    "breaking_bad_comments = []\n",
    "\n",
    "for post in top_posts:\n",
    "    for comment in post.comments:\n",
    "        if isinstance(comment, praw.models.MoreComments):\n",
    "            continue\n",
    "        if \"breaking bad\" in comment.body.lower():\n",
    "            breaking_bad_comments.append(comment.body)\n",
    "\n",
    "# Compute sentiment for each comment\n",
    "sentiments = [TextBlob(comment).sentiment.polarity for comment in breaking_bad_comments]\n",
    "\n",
    "# Visualize sentiment distribution\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.hist(sentiments, bins=30, color='skyblue', edgecolor='black')\n",
    "plt.title('Sentiment Distribution for \"Breaking Bad\" Comments')\n",
    "plt.xlabel('Polarity')\n",
    "plt.ylabel('Number of Comments')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

from wordcloud import WordCloud

# Generate word cloud
wordcloud = WordCloud(width=800, height=400, background_color='white').generate(all_texts)

# Display the word cloud using matplotlib
plt.figure(figsize=(10, 6))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis('off')
plt.title('Word Cloud of Top Terms in Netflix Subreddit')
plt.show()

# Extract mentions of shows/movies
show_mentions = [word for word in filtered_tokens if word not in custom_stopwords]

# Count show mentions
show_mention_counts = Counter(show_mentions)

# Visualize top mentioned shows/movies
top_shows = show_mention_counts.most_common(10)
show_names, show_counts = zip(*top_shows)

plt.figure(figsize=(12, 6))
plt.barh(show_names, show_counts, color='lightgreen')
plt.xlabel('Mentions')
plt.title('Top Mentioned Shows/Movies in Netflix Subreddit')
plt.gca().invert_yaxis()
plt.show()

# Define a list of popular shows/movies to analyze
popular_shows = ["stranger things", "the crown", "narcos", "the witcher"]

# Perform sentiment analysis for each show
show_sentiments = {}
for show in popular_shows:
    show_comments = [comment for comment in breaking_bad_comments if show in comment.lower()]
    sentiments = [TextBlob(comment).sentiment.polarity for comment in show_comments]
    show_sentiments[show] = sentiments

# Visualize sentiment comparison
plt.figure(figsize=(10, 6))
for show, sentiments in show_sentiments.items():
    plt.hist(sentiments, bins=20, alpha=0.5, label=show)
plt.title('Sentiment Comparison for Popular Shows/Movies Comments')
plt.xlabel('Polarity')
plt.ylabel('Number of Comments')
plt.legend()
plt.show()

from datetime import datetime

# Extract post timestamps and comment timestamps
post_timestamps = [datetime.fromtimestamp(post.created_utc) for post in top_posts]
comment_timestamps = [datetime.fromtimestamp(comment.created_utc) for post in top_posts for comment in post.comments if not isinstance(comment, praw.models.MoreComments)]

# Plot post and comment activity over time
plt.figure(figsize=(12, 6))
plt.plot(post_timestamps, range(len(post_timestamps)), label='Posts', color='orange')
plt.plot(comment_timestamps, range(len(comment_timestamps)), label='Comments', color='blue')
plt.xlabel('Time')
plt.ylabel('Activity Count')
plt.title('Post and Comment Activity Over Time')
plt.legend()
plt.show()

from sklearn.feature_extraction.text import CountVectorizer
from sklearn.decomposition import LatentDirichletAllocation

# Create a document-term matrix
vectorizer = CountVectorizer(max_df=0.85, stop_words=stop_words)
dtm = vectorizer.fit_transform(all_texts.split())

# Apply LDA
num_topics = 5  # Define the number of topics
lda_model = LatentDirichletAllocation(n_components=num_topics, random_state=42)
lda_model.fit(dtm)

# Display top words for each topic
for index, topic in enumerate(lda_model.components_):
    top_words = [vectorizer.get_feature_names()[i] for i in topic.argsort()[-10:]]
    print(f"Topic {index+1}: {', '.join(top_words)}")

# Calculate average upvotes and comments per post
average_upvotes = sum([post.score for post in top_posts]) / len(top_posts)
average_comments = sum([len(post.comments) for post in top_posts]) / len(top_posts)

print(f"Average Upvotes per Post: {average_upvotes:.2f}")
print(f"Average Comments per Post: {average_comments:.2f}")


import spacy

nlp = spacy.load('en_core_web_sm')

# Apply Named Entity Recognition
doc = nlp(all_texts)
entities = [(ent.text, ent.label_) for ent in doc.ents]

# Count and visualize named entities
entity_counts = Counter(entities)
top_entities = entity_counts.most_common(10)
entities, counts = zip(*top_entities)

plt.figure(figsize=(10, 6))
plt.barh(entities, counts, color='purple')
plt.xlabel('Counts')
plt.title('Top Named Entities in Netflix Subreddit')
plt.gca().invert_yaxis()
plt.show()
